{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c8ecdf00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAADkCAYAAADXee9ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfpklEQVR4nO3dfXBU1RnH8ScBsiKSDeElS0qCAUGZUtKRAkUoQskEkFKB2KKVUWcUFAItoMVixdfaFKhIYSKUaolMRZDa6BRHWowQtCUoqZYRJAJFCUJCZchuiLAEcvoH45aFe5bczT27m93vZ+b8kefsuXtyzc88WfbeTVJKKQEAAAAMSI72BgAAABC/aDYBAABgDM0mAAAAjKHZBAAAgDE0mwAAADCGZhMAAADG0GwCAADAGJpNAAAAGEOzCQAAAGNoNgEAAGBMW1MHLi4uliVLlkhNTY3k5ubKihUrZPDgwVdc19TUJEePHpWOHTtKUlKSqe0BEaeUkvr6esnMzJTkZOf+ziNrQDATWQs3ZyJkDfHJVs6UAevXr1cpKSnqj3/8o9qzZ4+aNm2aSktLU7W1tVdcW11drUSEwYjbUV1dTdYYjAgMp7LWkpyRNUa8j+bkzEizOXjwYFVYWBj4+vz58yozM1MVFRVdcW1dXV3UTxyDYXLU1dWRNQYjAsOprLUkZ2SNEe+jOTlz/D2bZ8+elcrKSsnLywvUkpOTJS8vT3bs2HHZ4/1+v/h8vsCor693ektATHHqn9HIGhCaE1mzmzMRsobE0pycOd5sfvnll3L+/HnJyMgIqmdkZEhNTc1ljy8qKhK32x0YWVlZTm8JiEtkDTDPbs5EyBpwqahfjb5gwQLxer2BUV1dHe0tAXGJrAGRQdaAYI5fjd6lSxdp06aN1NbWBtVra2vF4/Fc9niXyyUul8vpbQBxj6wB5tnNmQhZAy7l+CubKSkpMnDgQCkrKwvUmpqapKysTIYOHer00wEJi6wB5pEzwAFhX54Xwvr165XL5VIlJSVq7969avr06SotLU3V1NRcca3X6436lVUMhsnh9XrJGoMRgeFU1lqSM7LGiPfRnJwZaTaVUmrFihUqOztbpaSkqMGDB6uKiopmrSOUjHgfTjabZI3B0A8nsxZuzsgaI95Hc3KWpJRSEkN8Pp+43e5obwMwxuv1SmpqarS3QdYQ98gaYF5zchb1q9EBAAAQv2g2AQAAYAzNJgAAAIyh2QQAAIAxNJsAAAAwhmYTAAAAxtBsAgAAwBiaTQAAABhDswkAAABjaDYBAABgDM0mAAAAjKHZBAAAgDE0mwAAADCGZhMAAADG0GwCAADAGJpNAAAAGEOzCQAAAGNoNgEAAGAMzSYAAACModkEAACAMTSbAAAAMIZmEwAAAMa0dfqATzzxhDz55JNBteuvv1727dvn9FPFNY/Ho50bPXq0Zf0b3/iGds0XX3xhWR8yZIh2zU033aSdu/HGG7VzTiovL7esT5gwQbvm1KlTprYTU8ganKT7/8rLL7+sXXPzzTdb1quqqhzZU6wga0DLON5sioh885vflLfffvv/T9LWyNMACY+sAZFB1oDwGUlL27ZtQ74yB8AZZA2IDLIGhM/Iezb3798vmZmZ0qtXL7nzzjvl8OHD2sf6/X7x+XxBA0DzkDUgMsgaED7Hm80hQ4ZISUmJbN68WVauXCmHDh2S733ve1JfX2/5+KKiInG73YGRlZXl9JaAuETWgMgga0DLJCmllMknqKurk549e8rSpUvl3nvvvWze7/eL3+8PfO3z+QimcIHQ1+LxAiGv1yupqamOH5esoSXi8QIhsgaY15ycGX+Hc1pamvTt21cOHDhgOe9yucTlcpneRquzYsUK7VxBQUEEdxJdul9mjz32mHbN/PnzTW0npsVy1kaMGKGd69y5s2W9tLTU1HZgYdCgQZb1Dz74IMI7iX2xnDUgFhm/z+apU6fk4MGD0r17d9NPBSQ0sgZEBlkD7HG82XzooYekvLxcPvvsM/nnP/8pkyZNkjZt2sgdd9zh9FMBCY2sAZFB1oCWcfyf0Y8cOSJ33HGHnDhxQrp27SrDhw+XiooK6dq1q9NPBSQ0sgZEBlkDWsbxZnP9+vVOHxKABbIGRAZZA1qGz0YHAACAMTSbAAAAMIYPd42ysWPHWtZvueWWCO/EHt3tUD7++GPtmqlTp1rW27VrZ/v5I3WfTzhj5MiR2rk+ffpY1rn1kfOSk/WvL+Tk5FjWe/bsqV2TlJTU4j0htun+/7x06VLtGt19ordv365d89///tfexkLo0aOHdq5fv37auX379lnWJ02apF2Tm5trWW/fvr12jS43mzZt0q6ZPHmyZb2xsVG7JpbwyiYAAACModkEAACAMTSbAAAAMIZmEwAAAMbQbAIAAMAYrkaPgJSUFO3cE088YVkPdSWbk5RS2rlPP/1UO3fnnXda1g8cOKBd8/nnn1vWdecglIaGBttrED133XWXdm7Hjh0R3EliC/VZ3tOmTbOs/+lPf9Ku0V29i/jRu3dvy/oDDzygXdOmTRvLekFBgSN7iiVNTU2W9XPnzmnXtG1r3XqNHz9eu8blclnWuRodAAAACY9mEwAAAMbQbAIAAMAYmk0AAAAYQ7MJAAAAY2g2AQAAYAy3PoqAO+64Qzs3ePBgx57n1KlT2rnXXnvNsv7qq69q17z11lu295Cfn6+du//++20fT3eLowcffND2sRA9ycn8XRsLXnjhBdtr9u/fb2AnaC10t7eaOnWqds2MGTMs68OGDdOu0d0uKRx+v187t2nTJu1cRUWFZT3ULb5OnjxpWb/99tu1a2bNmqWdi1f8BgAAAIAxNJsAAAAwhmYTAAAAxtBsAgAAwBiaTQAAABhj+2r07du3y5IlS6SyslKOHTsmpaWlMnHixMC8Ukoef/xx+cMf/iB1dXUybNgwWblypfTp08fJfbcq69ev184tXLjQst6rVy/tGt2V2H/+85+1a6qrq7Vz4bj4v/nFXn75Ze2a9u3bW9Z1V5yLiMyfP9+yfuDAAf3m4kBrzdmAAQMs6xkZGRHeCay43W7ba7Zs2WJgJ7GjtWYt2jZs2GB7LtTdVzp16mRZz8zM1K45evSoZf3gwYPaNU7/7mjb1rqNev75520fa8eOHdq5M2fO2D5eLLH9ymZDQ4Pk5uZKcXGx5fzixYtl+fLlsmrVKtm5c6d06NBBxowZ0+pPFBBJ5AyIDLIGmGf7lc1x48bJuHHjLOeUUrJs2TJ59NFH5dZbbxURkbVr10pGRoa8/vrrIe87BeD/yBkQGWQNMM/R92weOnRIampqJC8vL1Bzu90yZMgQ7cvDfr9ffD5f0ACgF07ORMgaYBdZA5zhaLNZU1MjIpe/PysjIyMwd6mioiJxu92BkZWV5eSWgLgTTs5EyBpgF1kDnBH1q9EXLFggXq83MJy+kAXABWQNiAyyBgRztNn0eDwiIlJbWxtUr62tDcxdyuVySWpqatAAoBdOzkTIGmAXWQOcYfsCoVBycnLE4/FIWVmZfPvb3xYREZ/PJzt37pQZM2Y4+VStit/v187NnDnTsp6fn69ds3r1ast6qFsI6XTt2lU7d99992nnHn30Ucu67vZGIiJ79uyxrN92223aNVVVVdq5RBXLObvlllss66F+LuA83a2mcnJybB/riy++aOl2Wq1Yzlpr9P7770d7C47T/S7U3QYulFWrVmnnzp07Z/t4scR2s3nq1Kmg+1QdOnRIPvroI0lPT5fs7GyZM2eO/OpXv5I+ffpITk6OLFy4UDIzM7X3ZQRwOXIGRAZZA8yz3Wzu2rVLRo0aFfh63rx5IiJy9913S0lJicyfP18aGhpk+vTpUldXJ8OHD5fNmzfLVVdd5dyugThHzoDIIGuAebabzZEjR4pSSjuflJQkTz31lDz11FMt2hiQyMgZEBlkDTAv6lejAwAAIH7RbAIAAMAYR69Gh31///vfbdVDmTp1qnZu9uzZlvW+fftq17jdbtt7COX48eOW9VBX66N1uf76622v0d2lAOH77W9/a1nXXaUuIvLpp59a1uvr6x3ZE9Ba3Xjjjdq5hx9+2PbxNmzYYFl/5ZVXbB+rteCVTQAAABhDswkAAABjaDYBAABgDM0mAAAAjKHZBAAAgDE0mwAAADCGWx+1QnPnzrWsFxUVadekpKSY2k6zXfyRcBfbu3evds3TTz9tWV+6dKl2DbdSal0++OCDaG8h6lJTU7VzY8eOtayHutVZfn6+7T3oslZXV2f7WEBrpPs9OW3aNO0al8tlWT9z5ox2zerVqy3r586dC7G71o1XNgEAAGAMzSYAAACModkEAACAMTSbAAAAMIZmEwAAAMZwNXordPvtt1vWw7nivLq6Wju3bNky7dxnn31mWT99+rR2zS9/+UvL+rBhw7RrnnnmGcv6yZMntWtWrVqlnUPsSU9Pj8jz5ObmWtaTkpK0a/Ly8izrPXr00K7R5fDOO+/UrklO1v/dr8vUzp07tWt0d2Ro21b/v/zKykrtHJAIHnnkEcv6/fffb/tYCxcu1M5t3brV9vFaO17ZBAAAgDE0mwAAADCGZhMAAADG0GwCAADAGJpNAAAAGEOzCQAAAGOSlFLKzoLt27fLkiVLpLKyUo4dOyalpaUyceLEwPw999wjL730UtCaMWPGyObNm5t1fJ/PJ263286WEs7vfvc7y/rs2bO1azZu3GhZnzdvnnbNF198YW9jV5CammpZf/PNN7Vrhg8fblk/fvy4dk2vXr0s6w0NDSF2Fzler1d7Lr5mOmciZrL2/PPPW9ZD3Tqkrq7Osn748GEnthQwYMAAy3qoWx+dO3fOsv7VV19p1+zdu9eyHupWRbt27dLOlZeXW9Zra2u1a44cOWJZ79Spk3ZNOLdOi3XxnDWEZ8KECdq5V1991bLucrm0azZs2GBZv+uuu7RrGhsbtXOtUXNyZvuVzYaGBsnNzZXi4mLtY8aOHSvHjh0LjFdeecXu0wAJjZwBkUHWAPNs39R93LhxMm7cuJCPcblc4vF4wt4UkOjIGRAZZA0wz8h7Nrdt2ybdunWT66+/XmbMmCEnTpzQPtbv94vP5wsaAK7MTs5EyBoQLrIGtIzjzebYsWNl7dq1UlZWJosWLZLy8nIZN26cnD9/3vLxRUVF4na7AyMrK8vpLQFxx27ORMgaEA6yBrSc45+NfvHndn/rW9+SAQMGSO/evWXbtm0yevToyx6/YMGCoItUfD4fwQSuwG7ORMgaEA6yBrSc8Vsf9erVS7p06SIHDhywnHe5XJKamho0ANhzpZyJkDXACWQNsM/xVzYvdeTIETlx4oR0797d9FMljKeeesqy3q5dO+2aS2/d8TWnb28Uiu59Sz/4wQ+0a/bt22dZz8jI0K4ZOXKkZT3ULZZau1jJ2cyZMy3rn3/+uXbNTTfdZGo7QXS3Unr99de1az755BPLekVFhRNbapHp06dr57p27WpZ/89//mNqOwkjVrIGvd69e2vnnn32We2c7hZHp0+f1q554oknLOvxdnujlrLdbJ46dSroL7pDhw7JRx99JOnp6ZKeni5PPvmkFBQUiMfjkYMHD8r8+fPluuuukzFjxji6cSCekTMgMsgaYJ7tZnPXrl0yatSowNdfvy/l7rvvlpUrV8ru3bvlpZdekrq6OsnMzJT8/Hx5+umnQ94UFUAwcgZEBlkDzLPdbI4cOVJCfejQ3/72txZtCAA5AyKFrAHm8dnoAAAAMIZmEwAAAMYYvxodztN9eoXuSuBY5/V6tXOhrgLUufHGGy3r8Xw1eqxbtGhRtLcQd3T3eAzltddeM7ATIDq6detmWQ91xfl1111n+3nuvfde7VxVVZXt4yUiXtkEAACAMTSbAAAAMIZmEwAAAMbQbAIAAMAYmk0AAAAYQ7MJAAAAY7j1kUabNm0s6xd/rNmlrr76asv6u+++q11z8uRJexuLQ1OmTNHOde/e3fbxtm7d2pLtAHGrtLQ02lsAHPPcc89Z1n/4wx+GdbzHHnvMss4tw1qOVzYBAABgDM0mAAAAjKHZBAAAgDE0mwAAADCGZhMAAADGcDW6xi9+8QvL+tNPP237WDU1Ndq5FStWWNZXr16tXXPixAnbe4gFvXr1sqwvX75cu8blclnWGxoatGt2795tb2MAgJik+10sIjJ58mTbx3v11Ve1c4sWLbKsNzY22n4eBOOVTQAAABhDswkAAABjaDYBAABgDM0mAAAAjKHZBAAAgDG2ms2ioiIZNGiQdOzYUbp16yYTJ06UqqqqoMecOXNGCgsLpXPnznLNNddIQUGB1NbWOrppIN6RNSAyyBpgnq1bH5WXl0thYaEMGjRIzp07J4888ojk5+fL3r17pUOHDiIiMnfuXHnzzTdl48aN4na7ZdasWTJ58mT5xz/+YeQbMOW73/2uY8fyeDzauWeeecay/rOf/Uy75qGHHrKsr1+/Xrvm3Llz2rlwdO3a1bI+depU7ZoHHnjA1rFERE6dOmVZ//GPf6xd4/P5tHOtRSJlDc5LSkqyrPft21e7pqKiwtR2YhpZiw0jR460rP/0pz/VrtHdGm/Pnj3aNbrfnyLc4sgkW83m5s2bg74uKSmRbt26SWVlpYwYMUK8Xq+8+OKLsm7dOvn+978vIiJr1qyRfv36SUVFhaMNHBDPyBoQGWQNMK9F79n0er0iIpKeni4iIpWVldLY2Ch5eXmBx9xwww2SnZ0tO3bssDyG3+8Xn88XNAAEI2tAZJA1wHlhN5tNTU0yZ84cGTZsmPTv319ELnxSTkpKiqSlpQU9NiMjQ/spOkVFReJ2uwMjKysr3C0BcYmsAZFB1gAzwm42CwsL5eOPPw75PsHmWLBggXi93sCorq5u0fGAeEPWgMgga4AZYX02+qxZs2TTpk2yfft26dGjR6Du8Xjk7NmzUldXF/RXYG1trfYiGZfLpX2TL5DoyBoQGWQNMMdWs6mUktmzZ0tpaals27ZNcnJyguYHDhwo7dq1k7KyMikoKBARkaqqKjl8+LAMHTrUuV1HwMmTJx071nPPPaedGzt2rGW9X79+2jVr1661rE+aNEm7ZubMmZb1Tp06adeEOt68efMs6507d9au0dFdcS4iMmXKFMv6pW/qjzeJlDU4TyllWU9O5tbKlyJrkXPzzTdr51588UXLeqi7uRw+fNiy/uCDD2rXHDlyRDsHc2w1m4WFhbJu3Tp54403pGPHjoH3q7jdbmnfvr243W659957Zd68eZKeni6pqakye/ZsGTp0KFfsATaQNSAyyBpgnq1mc+XKlSJy+f2w1qxZI/fcc4+IXHgVLzk5WQoKCsTv98uYMWPk+eefd2SzQKIga0BkkDXAPNv/jH4lV111lRQXF0txcXHYmwISHVkDIoOsAebxBh4AAAAYQ7MJAAAAY2g2AQAAYExY99lMBM8++6xlffz48do1utsIDRo0SLvmgw8+sKxnZ2dr13To0MGyHupWRaNGjbKst2/fXrvG6fvElZWVWdanTZumXfPZZ585ugcgkYW6VU9JSUnkNoK41rt3b8v6mjVrtGuuvfZay/qePXu0a+677z7L+s6dO/WbQ1TwyiYAAACModkEAACAMTSbAAAAMIZmEwAAAMbQbAIAAMAYrkbX+Pe//21Z1139JiLywgsvWNaHDx+uXRNqzklpaWmOHq+8vNyy/vvf/167ZuPGjZb18+fPO7InABckJSVFewuIc1lZWdq5xYsXW9Z1V5yLiJw+fdqy/vOf/1y7hqvOWw9e2QQAAIAxNJsAAAAwhmYTAAAAxtBsAgAAwBiaTQAAABhDswkAAABjuPWRTaWlpdq5/fv3W9bnzJmjXdOzZ0/L+ujRo7Vrjh8/bll/9913tWt03n//fe2c7lZFIiKHDx+2rDc1NdneAwD73nrrLe3cj370owjuBPGsbVvrNmHZsmXaNZMmTbKsnzlzRrtmypQplvXNmzfrN4dWg1c2AQAAYAzNJgAAAIyh2QQAAIAxNJsAAAAwhmYTAAAA5igbfv3rX6vvfOc76pprrlFdu3ZVt956q9q3b1/QY26++WYlIkHj/vvvb/ZzeL3ey9YzGPE0vF4vWWMwIjDIWsvH6NGjLUdTU5N2nD592nKMHz9eO6L9fTLCH83Jma1XNsvLy6WwsFAqKipky5Yt0tjYKPn5+dLQ0BD0uGnTpsmxY8cCY/HixXaeBkh4ZA2IDLIGmGfrPpuX3u+qpKREunXrJpWVlTJixIhA/eqrrxaPx+PMDoEERNaAyCBrgHktes+m1+sVEZH09PSg+ssvvyxdunSR/v37y4IFC+Srr77SHsPv94vP5wsaAIKRNSAyyBrgvLA/QaipqUnmzJkjw4YNk/79+wfqP/nJT6Rnz56SmZkpu3fvlocffliqqqrkL3/5i+VxioqK5Mknnwx3G0DcI2tAZJA1wIwkpZQKZ+GMGTPkrbfekvfee0969Oihfdw777wjo0ePlgMHDkjv3r0vm/f7/eL3+wNf+3w+ycrKCmdLQKvg9XolNTW12Y8na0B4yFrL6T46ecuWLdo1F3/vF7vtttu0a9588017G0PMaE7Ownplc9asWbJp0ybZvn17yECKiAwZMkRERBtKl8slLpcrnG0AcY+sAZFB1gBzbDWbSimZPXu2lJaWyrZt2yQnJ+eKaz766CMREenevXtYGwQSEVkDIoOshdbU1GRZr62t1a4pLi62rPPqZeKy1WwWFhbKunXr5I033pCOHTtKTU2NiIi43W5p3769HDx4UNatWye33HKLdO7cWXbv3i1z586VESNGyIABA4x8A0A8ImtAZJA1wDxbzebKlStFRGTkyJFB9TVr1sg999wjKSkp8vbbb8uyZcukoaFBsrKypKCgQB599FHHNgwkArIGRAZZA8yz/c/ooWRlZUl5eXmLNgSArAGRQtYA8/hsdAAAABhDswkAAABjwr6pOwAAiG9bt261rCfClfhwDq9sAgAAwBiaTQAAABhDswkAAABjaDYBAABgDM0mAAAAjIm5ZvNKN9gFWrtY+RmPlX0ApsTKz3is7AMwoTk/3zHXbNbX10d7C4BRsfIzHiv7AEyJlZ/xWNkHYEJzfr6TVIz9ydXU1CRHjx6Vjh07SlJSkvh8PsnKypLq6mpJTU2N9vaignMQH+dAKSX19fWSmZkpycnR/zvv4qzV19e3+vPrhHj4OWupeDgHZC22xcPPmBNa+3mwk7OYu6l7cnKy9OjR47J6ampqq/yP4STOQes/B263O9pbCLg4a0lJSSLS+s+vUzgPrf8ckLXYxzm4oDWfh+bmLPp/8gEAACBu0WwCAADAmJhvNl0ulzz++OPicrmivZWo4RxwDkzj/F7AeeAcmMb55Rx8LZHOQ8xdIAQAAID4EfOvbAIAAKD1otkEAACAMTSbAAAAMIZmEwAAAMbEdLNZXFws1157rVx11VUyZMgQef/996O9JWO2b98uEyZMkMzMTElKSpLXX389aF4pJY899ph0795d2rdvL3l5ebJ///7obNaQoqIiGTRokHTs2FG6desmEydOlKqqqqDHnDlzRgoLC6Vz585yzTXXSEFBgdTW1kZpx/GDrP0fWbuArJlB1v6PrF2QCFmL2WZzw4YNMm/ePHn88cflX//6l+Tm5sqYMWPk+PHj0d6aEQ0NDZKbmyvFxcWW84sXL5bly5fLqlWrZOfOndKhQwcZM2aMnDlzJsI7Nae8vFwKCwuloqJCtmzZIo2NjZKfny8NDQ2Bx8ydO1f++te/ysaNG6W8vFyOHj0qkydPjuKuWz+yFoysXUDWnEfWgpG1CxIiaypGDR48WBUWFga+Pn/+vMrMzFRFRUVR3FVkiIgqLS0NfN3U1KQ8Ho9asmRJoFZXV6dcLpd65ZVXorDDyDh+/LgSEVVeXq6UuvA9t2vXTm3cuDHwmE8++USJiNqxY0e0ttnqkbXSwNdkjayZRNZKA1+TtcTKWky+snn27FmprKyUvLy8QC05OVny8vJkx44dUdxZdBw6dEhqamqCzofb7ZYhQ4bE9fnwer0iIpKeni4iIpWVldLY2Bh0Hm644QbJzs6O6/NgElkLRtbImilkLRhZS6ysxWSz+eWXX8r58+clIyMjqJ6RkSE1NTVR2lX0fP09J9L5aGpqkjlz5siwYcOkf//+InLhPKSkpEhaWlrQY+P5PJhG1oKRNbJmClkLRtYSK2tto70BwEphYaF8/PHH8t5770V7K0BcI2tAZCRy1mLylc0uXbpImzZtLrsaq7a2VjweT5R2FT1ff8+Jcj5mzZolmzZtkq1bt0qPHj0CdY/HI2fPnpW6urqgx8freYgEshaMrF1A1pxH1oKRtQsSJWsx2WympKTIwIEDpaysLFBramqSsrIyGTp0aBR3Fh05OTni8XiCzofP55OdO3fG1flQSsmsWbOktLRU3nnnHcnJyQmaHzhwoLRr1y7oPFRVVcnhw4fj6jxEElkLRtYuIGvOI2vByNoFCZO1KF+gpLV+/XrlcrlUSUmJ2rt3r5o+fbpKS0tTNTU10d6aEfX19erDDz9UH374oRIRtXTpUvXhhx+qzz//XCml1G9+8xuVlpam3njjDbV792516623qpycHHX69Oko79w5M2bMUG63W23btk0dO3YsML766qvAYx544AGVnZ2t3nnnHbVr1y41dOhQNXTo0CjuuvUja2SNrEUGWSNriZq1mG02lVJqxYoVKjs7W6WkpKjBgwerioqKaG/JmK1btyoRuWzcfffdSqkLt4lYuHChysjIUC6XS40ePVpVVVVFd9MOs/r+RUStWbMm8JjTp0+rmTNnqk6dOqmrr75aTZo0SR07dix6m44TZI2skbXIIGtkLRGzlqSUUmZfOwUAAECiisn3bAIAACA+0GwCAADAGJpNAAAAGEOzCQAAAGNoNgEAAGAMzSYAAACModkEAACAMTSbAAAAMIZmEwAAAMbQbAIAAMAYmk0AAAAYQ7MJAAAAY/4H5/gqZcphm9gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(8,12))\n",
    "axes[0].imshow(X_train[9401],cmap='gray')\n",
    "axes[1].imshow(X_train[2],cmap='gray')\n",
    "axes[2].imshow(X_train[3505],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_scaled = X_train / 255\n",
    "X_test_scaled = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28, 1]), TensorShape([10000, 28, 28, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_exp = expand_dims(X_train_scaled)\n",
    "X_test_scaled_exp = expand_dims(X_test_scaled)\n",
    "\n",
    "X_train_scaled_exp.shape, X_test_scaled_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "y_train_cat.shape, y_test_cat.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28,28,1), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    ### Model compilation\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = 'accuracy'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1758898734.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [70], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    params =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "params = 8*4*4 * 10 + 8\n",
    "print(params)\n",
    "\n",
    "params = 8*16* 3*3 + 16\n",
    "print(params)\n",
    "\n",
    "params =\n",
    "print(params)\n",
    "\n",
    "params = 10*10 + 10\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 14, 14, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0985 - accuracy: 0.9701 - val_loss: 0.0959 - val_accuracy: 0.9698\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0784 - accuracy: 0.9753 - val_loss: 0.0856 - val_accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled_exp,\n",
    "    y_train_cat,\n",
    "    validation_split = 0.3,\n",
    "    batch_size = 32,\n",
    "    callbacks = es,\n",
    "    epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE 1313 times - the number of batches used to optimize train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 782us/step - loss: 0.0665 - accuracy: 0.9784\n",
      "0.9783999919891357\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled_exp,y_test_cat)\n",
    "\n",
    "accuracy = results[1]\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
